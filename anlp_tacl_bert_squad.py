# -*- coding: utf-8 -*-
"""ANLP_TACL_BERT_Squad.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1L4G4vDVSalVI2ZqTW3Ti0UxA80ofjz2W

#ANLP Final Project

*   Paper Chosen : https://aclanthology.org/2022.findings-naacl.191/
*   This notebook analysis the comparision of Tacl-Bert and Bert to determine which model performs better on SQuAD Dataset



Team Memebers:


1.   Sai Sandeep Varma Mudundi (G01352322)
2.   Asra Naseem               (G01349680)
3.   Rajeev Priyatam Panchadula (G01333080)
"""

!pip install datasets transformers

cd /content/drive/MyDrive/TaCL-main/TaCL-main

from google.colab import drive
drive.mount('/content/drive')

#pre-trained TACL BERT-based model from paper
model_to_use = "cambridgeltl/tacl-bert-base-uncased"
#uncomment below line to use BERT
# model="bert-base-uncased"
batch_size = 12

"""## Load dataset"""

# Import the load_dataset and load_metric functions from the datasets library
from datasets import load_dataset, load_metric

# Load the SQuAD dataset using the load_dataset function
squad_dataset = load_dataset("squad")

# Print the loaded dataset to the console
print(squad_dataset)

# Access the first sample in the training split of the loaded dataset
first_sample = squad_dataset["train"][0]

"""##Preprocessing """

# Import the AutoTokenizer class from the transformers library
from transformers import AutoTokenizer

# Load a pre-trained tokenizer specified by the 'model' variable
tokenizer = AutoTokenizer.from_pretrained(model_to_use)

"""In this code, we're importing the transformers library, which is a popular library for working with pre-trained language models. We're then using the isinstance function to check that the tokenizer variable is an instance of the PreTrainedTokenizerFast class from the transformers library. This check assertion ensures that our tokenizer is a fast tokenizers"""

# Import the transformers library
import transformers

# Check that the 'tokenizer' variable is an instance of the PreTrainedTokenizerFast class
assert isinstance(tokenizer, transformers.PreTrainedTokenizerFast)

#testing tokenized input
# Define the input sentences
question = "What is the capital of France?"
response = "The capital of France is Paris."
#the output may vary depending on the model you have selected
# Apply the tokenizer to the input sentences
tokenized_input = tokenizer(question, response)

# Print the tokenized output to the console
print(tokenized_input)

# Set the maximum length of a feature to 384 (question and context)
max_length = 384

# Set the authorized overlap between two parts of the context when splitting is needed to 128
doc_stride = 128

# In question answering, dealing with very long documents requires a different preprocessing approach
# We can't simply truncate them as it may result in losing the answer we are looking for
# To handle this, we allow one long example in the dataset to give several input features, each shorter than the maximum length
# We also allow some overlap between the features we generate using the hyper-parameter doc_stride
# This ensures that the answer isn't lost if it lies at the point where we split a long context

#In this code cell, we're looping through each example in the training split of the SQuAD dataset until we find an example with a combined length (question and context) that exceeds the maximum length of 384. We're then storing that example in the example variable.

#We're using the len function and the tokenizer method to get the length of the input IDs for the example. We store this length in the original_length variable.

#Finally, we're using the tokenizer method with the max_length and truncation arguments to get the length of the truncated input IDs for the example. We store this length in the truncated_length variable.
# Loop through each example in the training split of the SQuAD dataset
# Stop once we find an example with a combined length (question and context) that exceeds the maximum length of 384
for i, example in enumerate(squad_dataset["train"]):
    if len(tokenizer(example["question"], example["context"])["input_ids"]) > 384:
        break

# Get the example with a combined length that exceeds the maximum length
example = squad_dataset["train"][i]

# Get the length of the input IDs for the example using the tokenizer
original_length = len(tokenizer(example["question"], example["context"])["input_ids"])

# Get the length of the truncated input IDs for the example using the tokenizer
truncated_length = len(tokenizer(example["question"], example["context"], max_length=max_length, truncation="only_second")["input_ids"])

"""In this code cell, we're using the tokenizer method to preprocess the example variable. We're passing in the question and context fields of the example as inputs, along with several other arguments:

max_length: Set to max_length, which is the maximum length of a feature.
truncation: Set to "only_second", which truncates the input to the maximum length of the second sequence (the context field in this case).
return_overflowing_tokens: Set to True, which causes the tokenizer to return the overflow tokens if the input exceeds the maximum length.
stride: Set to doc_stride, which is the authorized overlap between two parts of the context when splitting is needed.
"""

# Use the tokenizer to preprocess the example
# Set the maximum length to max_length, truncation to "only_second", return_overflowing_tokens to True, and stride to doc_stride
tokenized_example = tokenizer(
    example["question"],
    example["context"],
    max_length=max_length,
    truncation="only_second",
    return_overflowing_tokens=True,
    stride=doc_stride
)

[len(x) for x in tokenized_example["input_ids"]]

#decoding to see the overlap
for x in tokenized_example["input_ids"][:2]:
    print(tokenizer.decode(x))

tokenized_example = tokenizer(
    example["question"],
    example["context"],
    max_length=max_length,
    truncation="only_second",
    return_overflowing_tokens=True,
    return_offsets_mapping=True,
    stride=doc_stride
)
print(tokenized_example["offset_mapping"][0][:100])

"""In below code cell, we're using the tokenized_example variable to get the second input ID from the first example in the variable, and storing it in the first_token_id variable.

We're also using the tokenized_example variable to get the offsets for the second input ID from the first example, and storing it in the offsets variable.

Finally, we're using the tokenizer method to convert the first_token_id to its corresponding token, and using the offsets to get the original text corresponding to the token
"""

# Get the second input ID from the first example in tokenized_example
first_token_id = tokenized_example["input_ids"][0][1]

# Get the offsets for the second input ID from the first example in tokenized_example
offsets = tokenized_example["offset_mapping"][0][1]

# Use the tokenizer to convert the first_token_id to its corresponding token
# Use the offsets to get the original text corresponding to the token
print(tokenizer.convert_ids_to_tokens([first_token_id])[0], example["context"][offsets[0]:offsets[1]])

# Use the tokenizer to get the sequence IDs for the tokenized_example
sequence_ids = tokenized_example.sequence_ids()

# Print the sequence IDs to the console
print(sequence_ids)
#The sequence IDs represent the sequence to which each token belongs (i.e., the question or the context). The 0 sequence ID corresponds to the question, while the 1 sequence ID corresponds to the context.

"""In the below code cell, we're using the example variable to get the answers for the example. We then calculate the end character index of the answer using the answer_start and text fields.

We then find the token start index and token end index of the current answer span. We start at the beginning and end of the tokenized input, respectively, and iterate until we find a token in the context sequence.

We then check if the answer is outside the current span using the offset_mapping and sequence_ids outputs from the tokenizer. If it is, we label this feature with the CLS index. If not, we print a message indicating that the answer is not in this feature.

Finally, if the answer is within the current span, we move the token start index and token end index to the two ends of the answer, and print the start and end positions of the answer spa
"""

# Get the answers for the example
answers = example["answers"]

# Calculate the end character index of the answer
start_char = answers["answer_start"][0]
end_char = start_char + len(answers["text"][0])

# Find the token start index of the current answer span
# Keep incrementing the index until we find a token in the context sequence
token_start_index = 0
while sequence_ids[token_start_index] != 1:
    token_start_index += 1

# Find the token end index of the current answer span
# Keep decrementing the index until we find a token in the context sequence
token_end_index = len(tokenized_example["input_ids"][0]) - 1
while sequence_ids[token_end_index] != 1:
    token_end_index -= 1

# Check if the answer is outside the current span
# If it is, we label this feature with the CLS index
offsets = tokenized_example["offset_mapping"][0]
if (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):
    # Move the token_start_index and token_end_index to the two ends of the answer.
    # If the answer is the last word, we go after the last offset (edge case).
    while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:
        token_start_index += 1
    start_position = token_start_index - 1
    while offsets[token_end_index][1] >= end_char:
        token_end_index -= 1
    end_position = token_end_index + 1
    # Print the start and end positions of the answer span
    print(start_position, end_position)
else:
    print("The answer is not in this feature.")

"""The below cell defines a function prepare_train_features that takes in training examples and returns tokenized examples with start and end positions for the answer. The function tokenizes the examples with truncation and padding, and keeps overflows using a stride to handle long contexts. The function also removes left whitespace from the questions to prevent issues with context truncation when tokenizing. The tokenized examples are labeled with start and end positions for the answer, and impossible answers are labeled with the index of the CLS token."""

# Check if padding is applied on the right
pad_on_right = tokenizer.padding_side == "right"

# Define a function to prepare training features
def prepare_train_features(examples):
    # Remove left whitespace from the questions
    # This will help prevent issues with context truncation when tokenizing
    examples["question"] = [q.lstrip() for q in examples["question"]]

    # Tokenize the examples with truncation and padding
    # Keep overflows using a stride
    # Return offsets mapping
    tokenized_examples = tokenizer(
        examples["question" if pad_on_right else "context"],
        examples["context" if pad_on_right else "question"],
        truncation="only_second" if pad_on_right else "only_first",
        max_length=max_length,
        stride=doc_stride,
        return_overflowing_tokens=True,
        return_offsets_mapping=True,
        padding="max_length",
    )

    # Get the mapping of each feature to its corresponding example
    sample_mapping = tokenized_examples.pop("overflow_to_sample_mapping")

    # Get the offset mappings for each feature
    offset_mapping = tokenized_examples.pop("offset_mapping")

    # Label the examples with start and end positions for the answer
    tokenized_examples["start_positions"] = []
    tokenized_examples["end_positions"] = []

    # Loop over each feature and label the example with start and end positions for the answer
    for i, offsets in enumerate(offset_mapping):
        # Label impossible answers with the index of the CLS token
        input_ids = tokenized_examples["input_ids"][i]
        cls_index = input_ids.index(tokenizer.cls_token_id)

        # Get the sequence IDs for the current feature
        sequence_ids = tokenized_examples.sequence_ids(i)

        # Get the answers for the current feature
        sample_index = sample_mapping[i]
        answers = examples["answers"][sample_index]

        # If no answers are given, set the cls_index as answer
        if len(answers["answer_start"]) == 0:
            tokenized_examples["start_positions"].append(cls_index)
            tokenized_examples["end_positions"].append(cls_index)
        else:
            # Get the start and end character index of the answer in the text
            start_char = answers["answer_start"][0]
            end_char = start_char + len(answers["text"][0])

            # Get the start and end token index of the current span in the text
            token_start_index = 0
            while sequence_ids[token_start_index] != (1 if pad_on_right else 0):
                token_start_index += 1

            token_end_index = len(input_ids) - 1
            while sequence_ids[token_end_index] != (1 if pad_on_right else 0):
                token_end_index -= 1

            # Detect if the answer is out of the span (in which case this feature is labeled with the CLS index)
            if not (offsets[token_start_index][0] <= start_char and offsets[token_end_index][1] >= end_char):
                tokenized_examples["start_positions"].append(cls_index)
                tokenized_examples["end_positions"].append(cls_index)
            else:
                # Otherwise, move the token_start_index and token_end_index to the two ends of the answer
                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:
                    token_start_index += 1
                tokenized_examples["start_positions"].append(token_start_index - 1)
                while offsets[token_end_index][1] >= end_char:
                    token_end_index -= 1
                tokenized_examples["end_positions"].append(token_end_index + 1)

    return tokenized_examples

"""In the below code cell, we're using the prepare_train_features function to prepare the first 5 examples in the training split of the SQuAD dataset as features. We're storing these features in the features variable.

We're then using the map method of the squad_dataset object to apply the prepare_train_features function to the entire SQuAD dataset. We're setting the batched parameter to True to enable batch processing, and the remove_columns parameter to remove the original context and question columns from the output.
"""

# Prepare the first 5 examples in the training split of the SQuAD dataset as features
features = prepare_train_features(squad_dataset['train'][:5])

# Use the `map` method to apply `prepare_train_features` to the entire SQuAD dataset
# Set `batched` to `True` to enable batch processing
# Set `remove_columns` to remove the original `context` and `question` columns from the output
tokenized_datasets = squad_dataset.map(prepare_train_features, batched=True, remove_columns=squad_dataset["train"].column_names)

"""##Fine-tuning TaCL/BERT Models"""

from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer

model = AutoModelForQuestionAnswering.from_pretrained(model_to_use)

# Get the name of the model from the `model` variable
model_name = model_to_use.split("/")[-1]

# Set the training arguments for the fine-tuning step
args = TrainingArguments(
    f"{model_name}-finetuned-squad",                 # Output directory
    evaluation_strategy="epoch",                     # Evaluate every epoch
    learning_rate=3e-5,                              # Learning rate
    per_device_train_batch_size=batch_size,          # Batch size for training
    per_device_eval_batch_size=batch_size,           # Batch size for evaluation
    num_train_epochs=2,                              # Number of training epochs
    weight_decay=0.01,                               # Weight decay
    push_to_hub=False                                # Whether to push the model to the Hub
)

from transformers import default_data_collator
#will batch our processed examples together
data_collator = default_data_collator

# Create a `Trainer` object for the fine-tuning step
trainer = Trainer(
    model,                                               # The model to train
    args,                                                # Training arguments
    train_dataset=tokenized_datasets["train"],           # Training dataset
    eval_dataset=tokenized_datasets["validation"],       # Evaluation dataset
    data_collator=data_collator,                         # Data collator for creating minibatches
    tokenizer=tokenizer                                 # Tokenizer for preprocessing the data
)

# Train the model using the `trainer` object
trainer.train()

# Save the fine-tuned model
trainer.save_model("test-squad-trained")

"""## Analysis and Evaluation

The below code is optional, uncomment and use only if are trying to load the saved trainer from the above step. if you are running the notebook for the first time ignore the below cell.
"""

!CUDA_VISIBLE_DEVICES=0
from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer
#if you have saved the trained model,replace the path to avoid retraining
model = AutoModelForQuestionAnswering.from_pretrained("/content/drive/MyDrive/TaCL-main/TaCL-main/test-squad-trained")
# model = AutoModelForQuestionAnswering.from_pretrained("/content/drive/MyDrive/TaCL-main/test-squad-trained")
trainer = Trainer(
    model,
    args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
)

"""In the below code cell, we're using a for loop to get the first batch from the evaluation dataloader. We're then using a dictionary comprehension to move the batch to the device specified in the training arguments.

We're then using the model attribute of the trainer object to pass the batch to the model and get the output. We're using the torch.no_grad() context manager to disable gradient calculations during inference, which saves memory and speeds up the process.

Finally, we're using the keys method of the output dictionary to print the keys of the output dictionary. These keys correspond to the different types of output produced by the model during inference.
"""

import torch

# Get the first batch from the evaluation dataloader
for batch in trainer.get_eval_dataloader():
    break

# Move the batch to the device specified in the training arguments
batch = {k: v.to(trainer.args.device) for k, v in batch.items()}

# Pass the batch to the model to get the output
with torch.no_grad():
    output = trainer.model(**batch)

# Print the keys of the output dictionary
print(output.keys())

output.start_logits.shape, output.end_logits.shape
output.start_logits.argmax(dim=-1), output.end_logits.argmax(dim=-1)

import numpy as np
n_best_size = 20
start_logits = output.start_logits[0].cpu().numpy()
end_logits = output.end_logits[0].cpu().numpy()
# Gather the indices the best start/end logits:
start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()
end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()
valid_answers = []
for start_index in start_indexes:
    for end_index in end_indexes:
        if start_index <= end_index: # We need to refine that test to check the answer is inside the context
            valid_answers.append(
                {
                    "score": start_logits[start_index] + end_logits[end_index],
                    "text": "" # We need to find a way to get back the original substring corresponding to the answer in the context
                }
            )

def prepare_validation_features(examples):
    # Some questions have left whitespace that makes truncation fail; remove it
    examples["question"] = [q.lstrip() for q in examples["question"]]

    # Tokenize examples, truncating and padding with overflow using a stride
    # This results in one example possibly giving multiple features, with overlapping context
    tokenized_examples = tokenizer(
        examples["question" if pad_on_right else "context"],
        examples["context" if pad_on_right else "question"],
        truncation="only_second" if pad_on_right else "only_first",
        max_length=max_length,
        stride=doc_stride,
        return_overflowing_tokens=True,
        return_offsets_mapping=True,
        padding="max_length",
    )

    # Map each feature to its corresponding example
    sample_mapping = tokenized_examples.pop("overflow_to_sample_mapping")

    # Keep the example_id that gave us each feature and store offset mappings
    tokenized_examples["example_id"] = []

    # Iterate over all features and create a mapping of offsets for each
    for i in range(len(tokenized_examples["input_ids"])):
        # Get the sequence corresponding to that example to identify context vs question
        sequence_ids = tokenized_examples.sequence_ids(i)
        context_index = 1 if pad_on_right else 0

        # Find the example index containing this span of text
        sample_index = sample_mapping[i]
        tokenized_examples["example_id"].append(examples["id"][sample_index])

        # Set offset_mapping values to None if they're not part of the context
        tokenized_examples["offset_mapping"][i] = [
            (o if sequence_ids[k] == context_index else None)
            for k, o in enumerate(tokenized_examples["offset_mapping"][i])
        ]

    return tokenized_examples

# Process the validation data and prepare the validation features
processed_validation_data = squad_dataset["validation"].map(
    prepare_validation_features,
    batched=True,
    remove_columns=squad_dataset["validation"].column_names
)

# Generate raw predictions using the trained model and the validation features
raw_predictions = trainer.predict(processed_validation_data)

# Set the format of the validation features
processed_validation_data.set_format(
    type=processed_validation_data.format["type"],
    columns=list(processed_validation_data.features.keys())
)

# Set the maximum answer length
max_answer_length = 30

# Get start and end logits from model output
start_logits = output.start_logits[0].cpu().numpy()
end_logits = output.end_logits[0].cpu().numpy()

# Retrieve offset mapping and context for first example in validation data
offset_mapping = processed_validation_data[0]["offset_mapping"]
context = squad_dataset["validation"][0]["context"]

# Collect indices of top n_best_size start and end logits
top_start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()
top_end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()

# Find valid answers within context based on top start and end logits
valid_answers = []
for start_index in top_start_indexes:
    for end_index in top_end_indexes:
        # Check if start and end indexes are within the context
        if (
            start_index >= len(offset_mapping)
            or end_index >= len(offset_mapping)
            or offset_mapping[start_index] is None
            or offset_mapping[end_index] is None
        ):
            continue
        # Check if answer length is within max answer length
        if end_index < start_index or end_index - start_index + 1 > max_answer_length:
            continue
        # Retrieve the text of the answer and its score
        start_char = offset_mapping[start_index][0]
        end_char = offset_mapping[end_index][1]
        answer_text = context[start_char: end_char]
        answer_score = start_logits[start_index] + end_logits[end_index]
        valid_answers.append({"score": answer_score, "text": answer_text})

# Sort the valid answers by score and return top n_best_size
valid_answers = sorted(valid_answers, key=lambda x: x["score"], reverse=True)[:n_best_size]
valid_answers

import collections

# Get the validation examples and features.
examples = squad_dataset["validation"]
features = processed_validation_data

# Create a dictionary mapping each example ID to its corresponding index in `examples`.
example_id_to_index = {k: i for i, k in enumerate(examples["id"])}

# Create a dictionary to store the indices of features associated with each example.
features_per_example = collections.defaultdict(list)

# Loop over each feature and find its corresponding example ID.
for i, feature in enumerate(features):
    example_index = example_id_to_index[feature["example_id"]]
    
    # Append the index of this feature to the list of features associated with the example.
    features_per_example[example_index].append(i)

def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size=20, max_length=30):
    # Unpack the raw predictions into start and end scores.
    all_start_scores, all_end_scores = raw_predictions

    # Create a dictionary that maps each example ID to its corresponding feature indices.
    example_indices = collections.defaultdict(list)
    for i, feature in enumerate(features):
        example_indices[feature['example_id']].append(i)

    # Create an empty dictionary to store the predictions.
    predictions = {}

    # Loop over all the examples.
    print(f"Processing {len(examples)} examples split into {len(features)} features.")
    for example in examples:
        # Get the ID of the current example.
        example_id = example['id']

        # Get the feature indices that correspond to the current example ID.
        indices = example_indices[example_id]

        # Initialize variables to track the best answer and its score.
        best_answer = ''
        best_score = float('-inf')

        # Loop over all the features that correspond to the current example ID.
        for index in indices:
            # Get the feature at the current index.
            feature = features[index]

            # Get the start and end scores for the current feature.
            start_scores = all_start_scores[index]
            end_scores = all_end_scores[index]

            # Get the offset mapping for the current feature.
            offsets = feature['offset_mapping']

            # Loop over all possible combinations of start and end positions.
            for start in range(len(offsets)):
                for end in range(start, min(start + max_length, len(offsets))):
                    # Check if the current combination of start and end positions is valid.
                    if offsets[start] is None or offsets[end] is None:
                        continue
                    if start > end:
                        continue

                    # Compute the score for the current answer.
                    score = start_scores[start] + end_scores[end]

                    # If the current answer is the best we've seen so far, update our variables.
                    if score > best_score:
                        best_answer = example['context'][offsets[start][0]:offsets[end][1]]
                        best_score = score

        # Store the best answer (or an empty string if no answer was found) in the predictions dictionary.
        predictions[example_id] = best_answer if best_score > float('-inf') else ''
    # Return the predictions dictionary.
    return predictions

final_predictions = postprocess_qa_predictions(squad_dataset["validation"], processed_validation_data, raw_predictions.predictions)
# Load the appropriate metric based on the value of squad_v2.
metric = load_metric("squad")


formatted_predictions = [{"id": k, "prediction_text": v} for k, v in final_predictions.items()]

# Create a list of references for the validation dataset.
references = [{"id": ex["id"], "answers": ex["answers"]} for ex in squad_dataset["validation"]]

# Compute the metric using the formatted predictions and references.
metric.compute(predictions=formatted_predictions, references=references)

"""#Error Analysis"""

# Get the incorrect predictions
incorrect_predictions = []
for pred, ref in zip(formatted_predictions, references):
    if pred['prediction_text'] not in ref['answers']['text']:
        incorrect_predictions.append((pred['id'], pred['prediction_text'], ref['answers']['text']))
        
# Print the incorrect predictions
print("Incorrect Predictions:")
for i, (id_, pred, refs) in enumerate(incorrect_predictions[:3]):
    print(f"Example {i+1} - ID: {id_}")
    print()

incorrect_predictions = []
for i, prediction in enumerate(formatted_predictions):
    pred_text = prediction['prediction_text'].strip()
    example_id = prediction['id']
    example = next((x for x in references if x['id'] == example_id), None)
    if example is None:
        continue
    answer_texts = example['answers']['text']
    question_text = squad_dataset["validation"][i]["question"]
    if pred_text not in answer_texts:
        correct_text = answer_texts[0]
        incorrect_predictions.append({"question": question_text, "incorrect_answer": pred_text, "correct_answer": correct_text})
print("Number of incorrect predictions:", len(incorrect_predictions))
print()
for pred in incorrect_predictions[0:2000]:
    print("Question:", pred["question"])
    print("answer:", pred["incorrect_answer"])
    print("Correct answer:", pred["correct_answer"])
    print("--------------------------------------------------")

"""We conducted an error analysis on the TACL-BERT and BERT models, both trained on the SQuAD 1.0 dataset and identified several reasons for incorrect predictions. Some potential reasons include imprecise context understanding, over-specificity or insufficient specificity, difficulties handling ambiguity, misinterpretation of questions, and sensitivity to phrasing and paraphrasing.
TACL model:
1.	Context Understanding: The model fails to understand context accurately or comprehensively, leading to incorrect answers. Example: It answered "reciprocating steam engines" instead of "steam turbines" for 20th-century ship propulsion.
2.	Specificity Issues: The model generated overly specific or insufficiently specific answers. Example: It answered, "open loop system" instead of "open loop" and "hydrogen and helium" instead of "helium" in two different questions.
3.	Ambiguity Handling: The model struggles a bit with ambiguous questions or those with multiple valid answers. Example: It answered "Richard Trevithick and, separately, Oliver Evans" instead of "Oliver Evans" for the creator of an engine using high-pressure steam in 1801.
4.	Misinterpretation or Misrepresentation: The model misunderstands questions, leading to incorrect predictions. Example: It answered "edge railed rack and pinion Middleton Railway" instead of "Middleton Railway" for the railroad where Salamanca was used.
5.	Implicit Information Capture: The model struggles to capture implicit information or answer questions requiring deeper context understanding. Example: It answered "relatively little work is required to drive the pump" instead of the more specific answer about the Rankine cycle's compression stage.
6.	Sensitivity to Phrasing and Paraphrasing: The model could have difficulty with differently phrased or paraphrased questions due to biases or the inability to generalize. Example: It answered "turbine type steam engines" instead of "turbine" for the steam engine producing most electricity today.
BERT Model:
2.	Context understanding: BERT struggles with context, leading to incorrect answers. Example: Answering "the source of most of the chemical energy" instead of "chemical energy" for oxygen's role in combustion.
3.	Incorrect information extraction: BERT extracts wrong information from the text. Example: Answering "light sky-blue color" for the clarity of liquid oxygen, instead of "clear."
Areas where both models fail, but TACL performs better:
1.	Disambiguation: TACL outperforms BERT in disambiguating concepts. Example: TACL correctly answers "methane" as the primary component of natural gas, while BERT lists multiple components.
2.	Answer specificity: TACL provides more specific answers than BERT. Example: TACL answers "to generate ATP through cellular respiration" for mitochondria's role, while BERT answers "energy production."
Both models encounter challenges in handling ambiguity, as demonstrated by their inability to correctly answer a question about the creator of a high-pressure steam engine in 1801. Misinterpretation of questions also led to errors, such as when the model provided a more technical response to a question about the railroad on which Salamanca was used.
Despite these limitations, TACL-BERT generally outperforms BERT-base-uncased in areas such as context understanding, information extraction, disambiguation, and specificity. This improvement can be attributed to factors like better context comprehension, more accurate extraction, superior disambiguation capabilities, and the generation of more specific answers.
Additional Analysis from the paper: The authors could have investigated the model's performance on questions that require an understanding of implicit information or context. They could have performed a more granular analysis of the questions based on the level of context required, which might have helped identify specific areas where the model needs improvement. The authors could have examined the model's sensitivity to phrasing and paraphrasing by creating multiple versions of the same question with different linguistic structures. Analyzing the model's performance on these variations would help identify any biases in the training data or areas where the model struggles to generalize. They could have analyzed the model's performance across different data splits, including varying levels of question similarity between the training and evaluation sets. This would help identify any overfitting issues and provide insights into the model's ability to generalize to new questions.
Despite these limitations, TACL-BERT generally outperforms BERT-base-uncased in areas such as context understanding, information extraction, disambiguation, and specificity. The performance of TACL-BERT, both in terms of failures and improvements over the BERT model, can be attributed to several factors. First, the architecture of TACL-BERT allows for more effective context comprehension, which aids in understanding complex questions and producing more accurate answers. Second, the model's enhanced information extraction capabilities enable it to identify and focus on relevant textual information. Third, the TACL-BERT model exhibits superior disambiguation skills, allowing it to distinguish between similar concepts and determine the most appropriate response. Lastly, the model tends to generate more specific answers, which can be beneficial in certain cases but can also lead to over-specificity issues. These attributes contribute to TACL-BERT's overall performance, with the model outperforming BERT in several areas while still facing challenges in handling ambiguity and certain question types.
In conclusion, the errors in TACL-BERT and BERT models can stem from factors such as preprocessing, tokenization, model architecture, training data, and hyperparameter tuning. While both models have limitations in understanding context, extracting correct information, disambiguating concepts, and providing sufficiently specific answers, TACL-BERT typically performs better in these areas. Further investigation and optimization of these models can lead to a more accurate and reliable performance in question-answering tasks.

## Unit Test on TaCL-Bert vs BERT
"""

import torch
import numpy as np
from transformers import AutoTokenizer, AutoModel

# Load TACL model
tacl_tokenizer = AutoTokenizer.from_pretrained("cambridgeltl/tacl-bert-base-uncased")
tacl_model = AutoModel.from_pretrained("cambridgeltl/tacl-bert-base-uncased")

# Load BERT model
bert_tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
bert_model = AutoModel.from_pretrained("bert-base-uncased")

# Choose a sentence to compare
sentence = "The quick brown fox jumps over the lazy dog."

# Tokenize the sentence for both models
tacl_tokens = tacl_tokenizer.tokenize(sentence)
bert_tokens = bert_tokenizer.tokenize(sentence)

# Get the representations for each token in the sentence for both models
with torch.no_grad():
    tacl_outputs = tacl_model(torch.tensor([tacl_tokenizer.encode(sentence)]))[0][0]
    bert_outputs = bert_model(torch.tensor([bert_tokenizer.encode(sentence)]))[0][0]

# Calculate self-similarity for both models
def self_similarity(outputs):
    cos_sim = torch.nn.CosineSimilarity(dim=-1)
    similarity = []
    for i in range(outputs.shape[0]):
        for j in range(i+1, outputs.shape[0]):
            similarity.append(cos_sim(outputs[i], outputs[j]).item())
    return np.mean(similarity)

tacl_self_sim = self_similarity(tacl_outputs)
bert_self_sim = self_similarity(bert_outputs)

# Print results
print("TACL self-similarity:", tacl_self_sim)
print("BERT self-similarity:", bert_self_sim)
if tacl_self_sim < bert_self_sim:
    print("TACL has more discriminative token representations than BERT.")
else:
    print("BERT has more discriminative token representations than TACL.")

"""In the sample unit test, we used the sentence "The quick brown fox jumps over the lazy dog." as input for both the TACL and BERT models. This sentence is commonly used as it contains every letter of the English alphabet, making it a good choice for testing token representations. The test results showed that the TACL self-similarity was 0.3019, while the BERT self-similarity was 0.4077. TACL achieved better self-similarity scores due to its training process, which specifically optimizes the model for isotropic token representation spaces. In other words, TACL encourages more uniform and evenly distributed token embeddings, leading to better generalization and performance on downstream tasks. This is evident in the lower self-similarity score for TACL compared to BERT in the unit test, which suggests that the TACL model captures more nuanced and distinct features for each token.

##References


*  https://github.com/yxuansu/TaCL/tree/main/english_benchmark 
*   https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering
"""